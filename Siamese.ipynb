{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71xPbEjtBJtJ"
      },
      "source": [
        "Connecting to Colab and Importing libaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A_CIuqRFNFX",
        "outputId": "485df8e6-35ed-4bf3-faea-4bd1b784c52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_mItH-6E2bO"
      },
      "source": [
        "from os import walk\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOHxxhKKBXGO"
      },
      "source": [
        "Creating Dictionary with Keys as Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e63-4ES1FOvU",
        "outputId": "ed71180d-d51e-40bb-bb06-8cc122bc7596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dir='/content/drive/My Drive/Colab Notebooks/WaDaBa/'\n",
        "a_file = open(data_dir+\"category.pkl\", \"rb\")\n",
        "categories = pickle.load(a_file)\n",
        "categories.pop(5)\n",
        "categories.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([1, 2, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxboHrccBeVh"
      },
      "source": [
        "Function that returns Images from the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnF09ocyFoUJ"
      },
      "source": [
        "class WadabaDataset(Dataset):\n",
        "    def __init__(self, setSize, transform=None):\n",
        "        self.transform = transform\n",
        "        self.setSize = setSize\n",
        "    def __len__(self):\n",
        "        return self.setSize\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = None\n",
        "        img2 = None\n",
        "        label = None\n",
        "        if idx % 2 == 0: # select the same character for both images\n",
        "            category = random.choice([k for k in categories.keys()])\n",
        "            img1 = random.choice(categories[category])\n",
        "            img2 = random.choice(categories[category])\n",
        "            label = 0.0\n",
        "        else: # select a different character for both images\n",
        "            category1, category2 = random.choice([k for k in categories.keys()]), random.choice([k for k in categories.keys()])\n",
        "            while category1 == category2:\n",
        "              category1, category2 = random.choice([k for k in categories.keys()]), random.choice([k for k in categories.keys()])\n",
        "            label = 1.0\n",
        "            img1 = random.choice(categories[category1])\n",
        "            img2 = random.choice(categories[category2])\n",
        "        img1 = Image.open(data_dir + img1)\n",
        "        img2 = Image.open(data_dir + img2)\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        return img1, img2, torch.from_numpy(np.array([label], dtype=np.float32))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOW7zUBxBwv2"
      },
      "source": [
        "Function for N-way evaluation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMehwcSAa3F3"
      },
      "source": [
        "class NWayAll(Dataset):\n",
        "    def __init__(self, setSize,transform=None):\n",
        "        self.setSize = setSize\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return self.setSize\n",
        "    def __getitem__(self, idx):\n",
        "        # find one main image\n",
        "        imgName = all_files[idx]\n",
        "        index = int(imgName.rfind('_',3))\n",
        "        category = int(imgName[index+2:index+4])\n",
        "        mainImg = Image.open(data_dir + imgName)\n",
        "        # print(imgDir + '/' + imgName)\n",
        "        if self.transform:\n",
        "            mainImg = self.transform(mainImg)\n",
        "        \n",
        "        # find n numbers of distinct images, 1 in the same set as the main\n",
        "        testSet = []\n",
        "        label = 0\n",
        "        for i,j in enumerate([k for k in categories.keys()]):\n",
        "            testImgName = ''\n",
        "            if j == category:\n",
        "              label = i\n",
        "            testImgName = random.choice(categories[j])\n",
        "            testImg = Image.open(data_dir + testImgName)\n",
        "            if self.transform:\n",
        "                testImg = self.transform(testImg)\n",
        "            testSet.append(testImg)\n",
        "        # plt.imshow()\n",
        "        return category,mainImg, testSet, torch.from_numpy(np.array([label], dtype = int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrjmg72SB28p"
      },
      "source": [
        "List containg image categories and their corresponding image locations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuVZIz5pZRyz"
      },
      "source": [
        "all_files = []\n",
        "for k in categories.keys():\n",
        "  all_files.append(categories[k])\n",
        "all_files = [item for sublist in all_files for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InJGqAZDCEl1"
      },
      "source": [
        "Initializing Train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJQnjmthVmLZ"
      },
      "source": [
        "# choose a training dataset size and further divide it into train and validation set 80:20\n",
        "dataSize = 4000 # self-defined dataset size\n",
        "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
        "train_size = int(dataSize * TRAIN_PCT)\n",
        "val_size = dataSize - train_size\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "        transforms.Resize((105,105)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "wadabadataset = WadabaDataset(dataSize, transformations)\n",
        "train_set, val_set = random_split(wadabadataset, [train_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=25, num_workers=16)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, num_workers=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSG9fmbLCKlZ"
      },
      "source": [
        "Initializing Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rEyzWmBXgnJ"
      },
      "source": [
        "testSize = 200\n",
        "test_set = NWayOneShotEvalSet(testSize,transformations)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-t1uifbQ34"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PToBXQYCP0u"
      },
      "source": [
        "Creating Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eohTk7ANblom"
      },
      "source": [
        "#Different network structures, the commented out are the different experimenting structures\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Koch et al.\n",
        "        # Conv2d(input_channels, output_channels, kernel_size)\n",
        "        self.conv1 = nn.Conv2d(3, 64, 10) \n",
        "        self.conv2 = nn.Conv2d(64, 128, 7)  \n",
        "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fcOut = nn.Linear(4096, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def convs(self, x):\n",
        "\n",
        "        # Koch et al.\n",
        "        # out_dim = in_dim - kernel_size + 1  \n",
        "        #1, 105, 105\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        # 64, 96, 96\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 64, 48, 48\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        # 128, 42, 42\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 128, 21, 21\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        # 128, 18, 18\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 128, 9, 9\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        # 256, 6, 6\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.convs(x1)\n",
        "\n",
        "        # Koch et al.\n",
        "        x1 = x1.view(-1, 256 * 6 * 6)\n",
        "        x1 = self.sigmoid(self.fc1(x1))\n",
        "        \n",
        "        x2 = self.convs(x2)\n",
        "\n",
        "        # Koch et al.\n",
        "        x2 = x2.view(-1, 256 * 6 * 6)\n",
        "        x2 = self.sigmoid(self.fc1(x2))\n",
        "\n",
        "        x = torch.abs(x1 - x2)\n",
        "        x = self.fcOut(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoxQQ5KJCWEB"
      },
      "source": [
        "creating the network and couting the paramenters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WARrzBM2bvRe",
        "outputId": "5d48365e-5265-43cb-dafb-1f14622fb617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "siameseBaseLine = Net()\n",
        "siameseBaseLine = siameseBaseLine.to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model architecture:\\n\\n', model)\n",
        "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
        "    \n",
        "count_parameters(siameseBaseLine)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model architecture:\n",
            "\n",
            " Net(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (fcOut): Linear(in_features=4096, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "The model has 38,965,697 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Q__cC6CfvE"
      },
      "source": [
        "saving and loading checkpoint mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRe9qI-Wb2zq"
      },
      "source": [
        "saving and loading checkpoint mechanisms\n",
        "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
        "    if save_path==None:\n",
        "        return\n",
        "    save_path = save_path \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'val_loss': val_loss}\n",
        "\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    save_path = data_dir + 'Weights/5-siameseNet-batchnorm50.pt'\n",
        "    state_dict = torch.load(save_path)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    val_loss = state_dict['val_loss']\n",
        "    print(f'Model loaded from <== {save_path}')\n",
        "    \n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHiDMxoCjxC"
      },
      "source": [
        "training and validation after every epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjUOEBvoZ81S"
      },
      "source": [
        "import time\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "def train(model, train_loader, val_loader, num_epochs, criterion, save_name):\n",
        "    best_val_loss = float(\"Inf\") \n",
        "    #best_val_loss = 0.0729\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    cur_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        print(\"Starting epoch \" + str(epoch+1))\n",
        "        for img1, img2, labels in train_loader:\n",
        "            \n",
        "            # Forward\n",
        "            img1 = img1.to(device)\n",
        "            img2 = img2.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(img1, img2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for img1, img2, labels in val_loader:\n",
        "                img1 = img1.to(device)\n",
        "                img2 = img2.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(img1, img2)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
        "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
        "        train_loss.append(avg_train_loss)\n",
        "        validation_loss.append(avg_val_loss)\n",
        "        print(\"Time taken for epoch = \",(time.time()-start_time))\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
        "    \n",
        "    print(\"Finished Training\")  \n",
        "    return train_losses, val_losses  \n",
        "\n",
        "# evaluation metrics\n",
        "def eval(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        print('Starting Iteration')\n",
        "        count = 0\n",
        "        acc_category = {1:0,2:0,5:0,6:0,7:0}\n",
        "        for category,mainImg, imgSets, label in test_loader:\n",
        "            mainImg = mainImg.to(device)\n",
        "            predVal = 1\n",
        "            pred = -1\n",
        "            for i, testImg in enumerate(imgSets):\n",
        "                testImg = testImg.to(device)\n",
        "                output = model(mainImg, testImg)\n",
        "                if output < predVal:\n",
        "                    pred = i\n",
        "                    predVal = output\n",
        "            #print(label)\n",
        "            label = label.to(device)\n",
        "            if pred == label:\n",
        "                correct += 1\n",
        "                acc_category[category.numpy()[0]] += 1\n",
        "            count += 1\n",
        "            if count % 200 == 0:\n",
        "                print(\"Current Count is: {}\".format(count))\n",
        "                print('Accuracy on n way: {}'.format(correct/count))\n",
        "    return acc_category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnaFJJrNCnaQ"
      },
      "source": [
        "Running the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mZ40eOu6z2T",
        "outputId": "6c7866da-1217-46c6-ed71-fad4b231553d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 50\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "save_path = data_dir+'Weights/5-siameseNet-batchnorm50.pt'\n",
        "optimizer = optim.SGD(siameseBaseLine.parameters(), lr=0.001, momentum=0.9)\n",
        "train_losses, val_losses = train(siameseBaseLine, train_loader, val_loader, num_epochs, criterion, save_path)\n",
        "#load_model = Net().to(device)\n",
        "#optimizer = optim.SGD(load_model.parameters(), lr=0.001)\n",
        "#load_checkpoint(load_model,optimizer)\n",
        "#train_losses, val_losses = train(load_model, train_loader, val_loader, num_epochs, criterion, save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Epoch [1/50],Train Loss: 0.6887, Valid Loss: 0.68410975\n",
            "Time taken for epoch =  362.51566648483276\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 2\n",
            "Epoch [2/50],Train Loss: 0.6761, Valid Loss: 0.66835142\n",
            "Time taken for epoch =  351.4178624153137\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 3\n",
            "Epoch [3/50],Train Loss: 0.6321, Valid Loss: 0.60940485\n",
            "Time taken for epoch =  350.1337926387787\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 4\n",
            "Epoch [4/50],Train Loss: 0.5680, Valid Loss: 0.54943763\n",
            "Time taken for epoch =  348.8320252895355\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 5\n",
            "Epoch [5/50],Train Loss: 0.5262, Valid Loss: 0.46923578\n",
            "Time taken for epoch =  349.955858707428\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 6\n",
            "Epoch [6/50],Train Loss: 0.4803, Valid Loss: 0.47511437\n",
            "Time taken for epoch =  351.2259991168976\n",
            "Starting epoch 7\n",
            "Epoch [7/50],Train Loss: 0.4570, Valid Loss: 0.43741841\n",
            "Time taken for epoch =  352.3817262649536\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 8\n",
            "Epoch [8/50],Train Loss: 0.4441, Valid Loss: 0.42170266\n",
            "Time taken for epoch =  350.6717827320099\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 9\n",
            "Epoch [9/50],Train Loss: 0.4266, Valid Loss: 0.37683664\n",
            "Time taken for epoch =  354.0898518562317\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 10\n",
            "Epoch [10/50],Train Loss: 0.4010, Valid Loss: 0.38427690\n",
            "Time taken for epoch =  354.033509016037\n",
            "Starting epoch 11\n",
            "Epoch [11/50],Train Loss: 0.3908, Valid Loss: 0.35456542\n",
            "Time taken for epoch =  353.88626074790955\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 12\n",
            "Epoch [12/50],Train Loss: 0.3438, Valid Loss: 0.35938008\n",
            "Time taken for epoch =  355.00532245635986\n",
            "Starting epoch 13\n",
            "Epoch [13/50],Train Loss: 0.3279, Valid Loss: 0.30947642\n",
            "Time taken for epoch =  354.7026619911194\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 14\n",
            "Epoch [14/50],Train Loss: 0.3296, Valid Loss: 0.31366801\n",
            "Time taken for epoch =  354.1753816604614\n",
            "Starting epoch 15\n",
            "Epoch [15/50],Train Loss: 0.2921, Valid Loss: 0.24504817\n",
            "Time taken for epoch =  351.0069794654846\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 16\n",
            "Epoch [16/50],Train Loss: 0.2557, Valid Loss: 0.21542627\n",
            "Time taken for epoch =  354.273175239563\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 17\n",
            "Epoch [17/50],Train Loss: 0.2296, Valid Loss: 0.20805823\n",
            "Time taken for epoch =  355.2469003200531\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 18\n",
            "Epoch [18/50],Train Loss: 0.2078, Valid Loss: 0.17932899\n",
            "Time taken for epoch =  352.8742723464966\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 19\n",
            "Epoch [19/50],Train Loss: 0.1777, Valid Loss: 0.21501816\n",
            "Time taken for epoch =  351.6441226005554\n",
            "Starting epoch 20\n",
            "Epoch [20/50],Train Loss: 0.1499, Valid Loss: 0.19627217\n",
            "Time taken for epoch =  354.42997765541077\n",
            "Starting epoch 21\n",
            "Epoch [21/50],Train Loss: 0.1392, Valid Loss: 0.10847924\n",
            "Time taken for epoch =  353.69601368904114\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 22\n",
            "Epoch [22/50],Train Loss: 0.1383, Valid Loss: 0.12120731\n",
            "Time taken for epoch =  354.55355620384216\n",
            "Starting epoch 23\n",
            "Epoch [23/50],Train Loss: 0.1147, Valid Loss: 0.15524514\n",
            "Time taken for epoch =  351.1862270832062\n",
            "Starting epoch 24\n",
            "Epoch [24/50],Train Loss: 0.0985, Valid Loss: 0.09754402\n",
            "Time taken for epoch =  354.05532336235046\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 25\n",
            "Epoch [25/50],Train Loss: 0.0959, Valid Loss: 0.10048004\n",
            "Time taken for epoch =  354.717490196228\n",
            "Starting epoch 26\n",
            "Epoch [26/50],Train Loss: 0.0792, Valid Loss: 0.06949732\n",
            "Time taken for epoch =  357.6891756057739\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 27\n",
            "Epoch [27/50],Train Loss: 0.0692, Valid Loss: 0.05678239\n",
            "Time taken for epoch =  374.8730037212372\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 28\n",
            "Epoch [28/50],Train Loss: 0.0744, Valid Loss: 0.05941842\n",
            "Time taken for epoch =  368.06497025489807\n",
            "Starting epoch 29\n",
            "Epoch [29/50],Train Loss: 0.0717, Valid Loss: 0.04956238\n",
            "Time taken for epoch =  361.99359488487244\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 30\n",
            "Epoch [30/50],Train Loss: 0.0594, Valid Loss: 0.04277326\n",
            "Time taken for epoch =  362.85903906822205\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 31\n",
            "Epoch [31/50],Train Loss: 0.0723, Valid Loss: 0.06248156\n",
            "Time taken for epoch =  361.3948178291321\n",
            "Starting epoch 32\n",
            "Epoch [32/50],Train Loss: 0.0604, Valid Loss: 0.04610918\n",
            "Time taken for epoch =  358.844034910202\n",
            "Starting epoch 33\n",
            "Epoch [33/50],Train Loss: 0.0470, Valid Loss: 0.05104751\n",
            "Time taken for epoch =  361.2426540851593\n",
            "Starting epoch 34\n",
            "Epoch [34/50],Train Loss: 0.0404, Valid Loss: 0.03779645\n",
            "Time taken for epoch =  354.1915979385376\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 35\n",
            "Epoch [35/50],Train Loss: 0.0366, Valid Loss: 0.01873308\n",
            "Time taken for epoch =  357.2246015071869\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 36\n",
            "Epoch [36/50],Train Loss: 0.0467, Valid Loss: 0.04458685\n",
            "Time taken for epoch =  357.3531074523926\n",
            "Starting epoch 37\n",
            "Epoch [37/50],Train Loss: 0.0459, Valid Loss: 0.06399945\n",
            "Time taken for epoch =  351.5968265533447\n",
            "Starting epoch 38\n",
            "Epoch [38/50],Train Loss: 0.0519, Valid Loss: 0.03502262\n",
            "Time taken for epoch =  354.16030192375183\n",
            "Starting epoch 39\n",
            "Epoch [39/50],Train Loss: 0.0362, Valid Loss: 0.02401909\n",
            "Time taken for epoch =  351.96669912338257\n",
            "Starting epoch 40\n",
            "Epoch [40/50],Train Loss: 0.0253, Valid Loss: 0.01906044\n",
            "Time taken for epoch =  347.9285683631897\n",
            "Starting epoch 41\n",
            "Epoch [41/50],Train Loss: 0.0213, Valid Loss: 0.01985816\n",
            "Time taken for epoch =  347.96172642707825\n",
            "Starting epoch 42\n",
            "Epoch [42/50],Train Loss: 0.0218, Valid Loss: 0.02139743\n",
            "Time taken for epoch =  346.16486978530884\n",
            "Starting epoch 43\n",
            "Epoch [43/50],Train Loss: 0.0224, Valid Loss: 0.01515858\n",
            "Time taken for epoch =  353.2654755115509\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 44\n",
            "Epoch [44/50],Train Loss: 0.0156, Valid Loss: 0.01151781\n",
            "Time taken for epoch =  357.9338972568512\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 45\n",
            "Epoch [45/50],Train Loss: 0.0172, Valid Loss: 0.01717034\n",
            "Time taken for epoch =  359.1017804145813\n",
            "Starting epoch 46\n",
            "Epoch [46/50],Train Loss: 0.0150, Valid Loss: 0.01512357\n",
            "Time taken for epoch =  355.99884390830994\n",
            "Starting epoch 47\n",
            "Epoch [47/50],Train Loss: 0.0169, Valid Loss: 0.01088507\n",
            "Time taken for epoch =  350.083922624588\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 48\n",
            "Epoch [48/50],Train Loss: 0.0122, Valid Loss: 0.00947164\n",
            "Time taken for epoch =  358.8203523159027\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Starting epoch 49\n",
            "Epoch [49/50],Train Loss: 0.0184, Valid Loss: 0.01292064\n",
            "Time taken for epoch =  355.55191111564636\n",
            "Starting epoch 50\n",
            "Epoch [50/50],Train Loss: 0.0120, Valid Loss: 0.00838412\n",
            "Time taken for epoch =  352.2850184440613\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/5-siameseNet-batchnorm50.pt\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZNzn9JdC0pE"
      },
      "source": [
        "Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wqNAOGldfof"
      },
      "source": [
        "testSize_all = 4000\n",
        "test_set_all = NWayAll(testSize_all,transformations)\n",
        "test_loader_all = torch.utils.data.DataLoader(test_set_all, batch_size = 1, num_workers = 2, shuffle=True)\n",
        "import torch.optim as optim\n",
        "load_model = Net().to(device)\n",
        "load_optimizer = optim.SGD(load_model.parameters(), lr=0.001)\n",
        "\n",
        "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
        "\n",
        "print(best_val_loss)\n",
        "acc_category = eval(load_model, test_loader_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt8ykEobwhgT"
      },
      "source": [
        "for k in acc_category.keys():\n",
        "  print(k,\":\",acc_category[k]/len(categories[k]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}