{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Triplet Loss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WvDu3GVDcKT"
      },
      "source": [
        "\n",
        "Connecting to Colab and Importing libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8kIuVK1TIhs",
        "outputId": "3f5cc3ae-76ad-4bde-c929-081c99ff6edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ039D3tTPxd"
      },
      "source": [
        "from os import walk\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwEYGUX4DjnB"
      },
      "source": [
        "Creating Dictionary with Keys as Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQTrVJOhTSVT",
        "outputId": "98dc55db-aae2-4ec2-b1e6-ee64b0e9e45b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_dir='/content/drive/My Drive/Colab Notebooks/WaDaBa/'\n",
        "a_file = open(data_dir+\"category.pkl\", \"rb\")\n",
        "categories = pickle.load(a_file)\n",
        "#categories.pop(7)\n",
        "categories.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([1, 5, 2, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diOnTMtVDnHL"
      },
      "source": [
        "\n",
        "Function that returns Images from the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpfVNyxNTUeK"
      },
      "source": [
        "class WadabaDataset(Dataset):\n",
        "    def __init__(self, setSize, transform=None):\n",
        "        self.transform = transform\n",
        "        self.setSize = setSize\n",
        "    def __len__(self):\n",
        "        return self.setSize\n",
        "    def __getitem__(self, idx):\n",
        "        img1 = None\n",
        "        img2 = None\n",
        "        img3 = None\n",
        "        category1 = random.choice([k for k in categories.keys()])\n",
        "        anchor_img = random.choice(categories[category1])\n",
        "        pos_img = random.choice(categories[category1])\n",
        "        while anchor_img == pos_img:\n",
        "            pos_img = random.choice(categories[category1])\n",
        "        category2 = random.choice([k for k in categories.keys()])\n",
        "        while category1 == category2:\n",
        "            category2 = random.choice([k for k in categories.keys()])\n",
        "        neg_img = random.choice(categories[category2])\n",
        "\n",
        "        anchor_img = Image.open(data_dir + anchor_img)\n",
        "        pos_img = Image.open(data_dir + pos_img)\n",
        "        neg_img = Image.open(data_dir + neg_img)\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            pos_img = self.transform(pos_img)\n",
        "            neg_img = self.transform(neg_img)\n",
        "        return anchor_img, pos_img,neg_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTjnInuZDt-Q"
      },
      "source": [
        "Function for N-way evaluation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0uwMU3kBi0e"
      },
      "source": [
        "class NWayOneShotEvalSet(Dataset):\n",
        "    def __init__(self, setSize,transform=None):\n",
        "        self.setSize = setSize\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return self.setSize\n",
        "    def __getitem__(self, idx):\n",
        "        # find one main image\n",
        "        category = random.choice([k for k in categories.keys()])\n",
        "        imgName = random.choice(categories[category])\n",
        "        mainImg = Image.open(data_dir + imgName)\n",
        "        # print(imgDir + '/' + imgName)\n",
        "        if self.transform:\n",
        "            mainImg = self.transform(mainImg)\n",
        "        \n",
        "        # find n numbers of distinct images, 1 in the same set as the main\n",
        "        testSet = []\n",
        "        label = 0\n",
        "        for i,j in enumerate([k for k in categories.keys()]):\n",
        "            testImgName = ''\n",
        "            if j == category:\n",
        "              label = i\n",
        "            testImgName = random.choice(categories[j])\n",
        "            testImg = Image.open(data_dir + testImgName)\n",
        "            if self.transform:\n",
        "                testImg = self.transform(testImg)\n",
        "            testSet.append(testImg)\n",
        "        # plt.imshow()\n",
        "        return mainImg, testSet, torch.from_numpy(np.array([label], dtype = int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF5NGRV9DwCf"
      },
      "source": [
        "Custom Loss function for triplet loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIr30bV5fm-I"
      },
      "source": [
        "def loss_fn(anchor_emb,pos_emb,neg_emb,margin):\n",
        "    #pos_dist = (anchor_emb - pos_emb).pow(2).sum(1)\n",
        "    pos_dist = torch.pow((anchor_emb - pos_emb),2).sum(1)\n",
        "    #neg_dist = (anchor_emb - neg_emb.pow(2)).sum(1)\n",
        "    neg_dist = torch.pow((anchor_emb - neg_emb),2).sum(1)\n",
        "    loss = torch.relu(pos_dist - neg_dist + margin)\n",
        "    return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1zYQkZ7O0Df"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivqq2ODsD4tq"
      },
      "source": [
        "\n",
        "Creating Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH8zzuAKkzUn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Koch et al.\n",
        "        # Conv2d(input_channels, output_channels, kernel_size)\n",
        "        self.conv1 = nn.Conv2d(3, 64, 10) \n",
        "        self.conv2 = nn.Conv2d(64, 128, 7)  \n",
        "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fc2 = nn.Linear(4096,128)\n",
        "        #self.fcOut = nn.Linear(4096, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def convs(self, x):\n",
        "\n",
        "        # Koch et al.\n",
        "        # out_dim = in_dim - kernel_size + 1  \n",
        "        #1, 105, 105\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        # 64, 96, 96\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 64, 48, 48\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        # 128, 42, 42\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 128, 21, 21\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        # 128, 18, 18\n",
        "        x = F.max_pool2d(x, (2,2))\n",
        "        # 128, 9, 9\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        # 256, 6, 6\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x1 = self.convs(x1)\n",
        "\n",
        "        # Koch et al.\n",
        "        x1 = x1.view(-1, 256 * 6 * 6)\n",
        "        x1 = self.sigmoid(self.fc1(x1))\n",
        "        x1 = self.sigmoid(self.fc2(x1))\n",
        "        \n",
        "        return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eux0eLdEAYe"
      },
      "source": [
        "\n",
        "creating the network and couting the paramenters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y39rzsCjlenA",
        "outputId": "4d5f25d6-82f3-4826-ab0c-7d1b2faf904f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "Triplet_Baseline = Net()\n",
        "Triplet_Baseline = Triplet_Baseline.to(device)\n",
        "\n",
        "def count_parameters(model):\n",
        "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model architecture:\\n\\n', model)\n",
        "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
        "    \n",
        "count_parameters(Triplet_Baseline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "The model architecture:\n",
            "\n",
            " Net(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(10, 10), stride=(1, 1))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
            "  (conv3): Conv2d(128, 128, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (fc2): Linear(in_features=4096, out_features=128, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "The model has 39,486,016 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBVpUe7xES3X"
      },
      "source": [
        "\n",
        "saving and loading checkpoint mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDJhZwXOyV4"
      },
      "source": [
        "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
        "    if save_path==None:\n",
        "        return\n",
        "    save_path = save_path \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'val_loss': val_loss}\n",
        "\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(model, optimizer):\n",
        "    save_path = data_dir + 'Weights/temp-TripletNet2-batchnorm50.pt'\n",
        "    state_dict = None\n",
        "    if torch.cuda.is_available():\n",
        "        state_dict = torch.load(save_path)\n",
        "    else:\n",
        "        state_dict = torch.load(save_path,map_location=torch.device('cpu')) \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    val_loss = state_dict['val_loss']\n",
        "    print(f'Model loaded from <== {save_path}')\n",
        "    \n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMzYfIVAEYkO"
      },
      "source": [
        "Initializing Train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-F00asVPKYS"
      },
      "source": [
        "# choose a training dataset size and further divide it into train and validation set 80:20\n",
        "dataSize = 4000 # self-defined dataset size\n",
        "TRAIN_PCT = 0.8 # percentage of entire dataset for training\n",
        "train_size = int(dataSize * TRAIN_PCT)\n",
        "val_size = dataSize - train_size\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "        transforms.Resize((105,105)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "wadabadataset = WadabaDataset(dataSize, transformations)\n",
        "train_set, val_set = random_split(wadabadataset, [train_size, val_size])\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, num_workers=16)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=20, num_workers=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KPoaQoWEgc-"
      },
      "source": [
        "Training and Validation after every epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l60tbfcsR1eb"
      },
      "source": [
        "# training and validation after every epoch\n",
        "import time\n",
        "\n",
        "def train(model, train_loader, val_loader, num_epochs, save_name):\n",
        "    best_val_loss = float(\"Inf\") \n",
        "    #best_val_loss = 0.00254\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    cur_step = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        print(\"Starting epoch \" + str(epoch+1))\n",
        "        for anchor_img, pos_img, neg_img in train_loader:\n",
        "            \n",
        "            # Forward\n",
        "            anchor_img = anchor_img.to(device)\n",
        "            pos_img = pos_img.to(device)\n",
        "            neg_img = neg_img.to(device)\n",
        "            anchor_emb = model(anchor_img)\n",
        "            pos_emb = model(pos_img)\n",
        "            neg_emb = model(neg_img)\n",
        "            loss = loss_fn(anchor_emb, pos_emb, neg_emb,0.8)\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for anchor_img, pos_img, neg_img in val_loader:\n",
        "                anchor_img = anchor_img.to(device)\n",
        "                pos_img = pos_img.to(device)\n",
        "                neg_img = neg_img.to(device)\n",
        "                anchor_emb = model(anchor_img)\n",
        "                pos_emb = model(pos_img)\n",
        "                neg_emb = model(neg_img)\n",
        "                loss = loss_fn(anchor_emb, pos_emb, neg_emb,0.8)\n",
        "                val_running_loss += loss.item()\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
        "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
        "        train_loss.append(avg_train_loss)\n",
        "        validation_loss.append(avg_val_loss)\n",
        "        print(\"Time taken for epoch = \",(time.time()-start_time))\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint(save_name, model, optimizer, best_val_loss)\n",
        "    \n",
        "    print(\"Finished Training\")  \n",
        "    return train_losses, val_losses  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5MOL0j8EnKl"
      },
      "source": [
        "Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfV2D5WZrm0f",
        "outputId": "5de70b36-aba5-42a4-afb4-6bfb7467ce49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 30\n",
        "save_path = data_dir+'Weights/temp-TripletNet2-batchnorm50.pt'\n",
        "#optimizer = optim.SGD(Triplet_Baseline.parameters(), lr=0.01, momentum=0.9)\n",
        "#train_losses, val_losses = train(Triplet_Baseline, train_loader, val_loader, num_epochs, save_path)\n",
        "load_model = Net().to(device)\n",
        "optimizer = optim.SGD(load_model.parameters(), lr=0.001,momentum=0.9)\n",
        "load_checkpoint(load_model,optimizer)\n",
        "train_losses, val_losses = train(load_model, train_loader, val_loader, num_epochs,save_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/temp-TripletNet2-batchnorm50.pt\n",
            "Starting epoch 1\n",
            "Epoch [1/30],Train Loss: 0.0062, Valid Loss: 0.00267426\n",
            "Time taken for epoch =  579.0906186103821\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/temp-TripletNet2-batchnorm50.pt\n",
            "Starting epoch 2\n",
            "Epoch [2/30],Train Loss: 0.0085, Valid Loss: 0.00582511\n",
            "Time taken for epoch =  543.7220079898834\n",
            "Starting epoch 3\n",
            "Epoch [3/30],Train Loss: 0.0080, Valid Loss: 0.00500970\n",
            "Time taken for epoch =  536.4187712669373\n",
            "Starting epoch 4\n",
            "Epoch [4/30],Train Loss: 0.0107, Valid Loss: 0.02155743\n",
            "Time taken for epoch =  534.584451675415\n",
            "Starting epoch 5\n",
            "Epoch [5/30],Train Loss: 0.0066, Valid Loss: 0.00427334\n",
            "Time taken for epoch =  533.8446831703186\n",
            "Starting epoch 6\n",
            "Epoch [6/30],Train Loss: 0.0072, Valid Loss: 0.01187497\n",
            "Time taken for epoch =  535.5806725025177\n",
            "Starting epoch 7\n",
            "Epoch [7/30],Train Loss: 0.0077, Valid Loss: 0.00648856\n",
            "Time taken for epoch =  539.8410441875458\n",
            "Starting epoch 8\n",
            "Epoch [8/30],Train Loss: 0.0055, Valid Loss: 0.00089558\n",
            "Time taken for epoch =  531.9776830673218\n",
            "Model saved to ==> /content/drive/My Drive/Colab Notebooks/WaDaBa/Weights/temp-TripletNet2-batchnorm50.pt\n",
            "Starting epoch 9\n",
            "Epoch [9/30],Train Loss: 0.0093, Valid Loss: 0.00324755\n",
            "Time taken for epoch =  535.3356673717499\n",
            "Starting epoch 10\n",
            "Epoch [10/30],Train Loss: 0.0071, Valid Loss: 0.00363948\n",
            "Time taken for epoch =  531.6539223194122\n",
            "Starting epoch 11\n",
            "Epoch [11/30],Train Loss: 0.0075, Valid Loss: 0.00406221\n",
            "Time taken for epoch =  535.8553485870361\n",
            "Starting epoch 12\n",
            "Epoch [12/30],Train Loss: 0.0070, Valid Loss: 0.01325142\n",
            "Time taken for epoch =  537.2674782276154\n",
            "Starting epoch 13\n",
            "Epoch [13/30],Train Loss: 0.0050, Valid Loss: 0.00886554\n",
            "Time taken for epoch =  539.6862347126007\n",
            "Starting epoch 14\n",
            "Epoch [14/30],Train Loss: 0.0054, Valid Loss: 0.00773363\n",
            "Time taken for epoch =  538.9759922027588\n",
            "Starting epoch 15\n",
            "Epoch [15/30],Train Loss: 0.0063, Valid Loss: 0.00770465\n",
            "Time taken for epoch =  542.1845064163208\n",
            "Starting epoch 16\n",
            "Epoch [16/30],Train Loss: 0.0055, Valid Loss: 0.00331053\n",
            "Time taken for epoch =  529.7438552379608\n",
            "Starting epoch 17\n",
            "Epoch [17/30],Train Loss: 0.0058, Valid Loss: 0.00817699\n",
            "Time taken for epoch =  543.6571133136749\n",
            "Starting epoch 18\n",
            "Epoch [18/30],Train Loss: 0.0150, Valid Loss: 0.00524469\n",
            "Time taken for epoch =  543.2935996055603\n",
            "Starting epoch 19\n",
            "Epoch [19/30],Train Loss: 0.0075, Valid Loss: 0.00280384\n",
            "Time taken for epoch =  535.8823392391205\n",
            "Starting epoch 20\n",
            "Epoch [20/30],Train Loss: 0.0084, Valid Loss: 0.00599178\n",
            "Time taken for epoch =  529.489807844162\n",
            "Starting epoch 21\n",
            "Epoch [21/30],Train Loss: 0.0038, Valid Loss: 0.00178860\n",
            "Time taken for epoch =  535.3919620513916\n",
            "Starting epoch 22\n",
            "Epoch [22/30],Train Loss: 0.0069, Valid Loss: 0.00429390\n",
            "Time taken for epoch =  536.7129406929016\n",
            "Starting epoch 23\n",
            "Epoch [23/30],Train Loss: 0.0042, Valid Loss: 0.00497682\n",
            "Time taken for epoch =  538.6223220825195\n",
            "Starting epoch 24\n",
            "Epoch [24/30],Train Loss: 0.0043, Valid Loss: 0.00618134\n",
            "Time taken for epoch =  540.1439251899719\n",
            "Starting epoch 25\n",
            "Epoch [25/30],Train Loss: 0.0045, Valid Loss: 0.00911911\n",
            "Time taken for epoch =  538.3329412937164\n",
            "Starting epoch 26\n",
            "Epoch [26/30],Train Loss: 0.0062, Valid Loss: 0.00461899\n",
            "Time taken for epoch =  532.504079580307\n",
            "Starting epoch 27\n",
            "Epoch [27/30],Train Loss: 0.0078, Valid Loss: 0.00376333\n",
            "Time taken for epoch =  537.8387033939362\n",
            "Starting epoch 28\n",
            "Epoch [28/30],Train Loss: 0.0043, Valid Loss: 0.00490535\n",
            "Time taken for epoch =  525.6384947299957\n",
            "Starting epoch 29\n",
            "Epoch [29/30],Train Loss: 0.0058, Valid Loss: 0.00426861\n",
            "Time taken for epoch =  520.142599105835\n",
            "Starting epoch 30\n",
            "Epoch [30/30],Train Loss: 0.0014, Valid Loss: 0.00325609\n",
            "Time taken for epoch =  519.2737722396851\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "760ECDO7ExvL"
      },
      "source": [
        "Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNA9t4vD_faO"
      },
      "source": [
        "# evaluation metrics\n",
        "def eval(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        print('Starting Iteration')\n",
        "        count = 0\n",
        "        #acc_category = {1:0,2:0,5:0,6:0,7:0}\n",
        "        for mainImg, imgSets, label in test_loader:\n",
        "            mainImg = mainImg.to(device)\n",
        "            predVal = float('inf')\n",
        "            pred = -1\n",
        "            for i, testImg in enumerate(imgSets):\n",
        "                testImg = testImg.to(device)\n",
        "                output = torch.abs(model(mainImg) - model(testImg))\n",
        "                output = torch.pow((model(mainImg) - model(testImg)),2).sum(1)\n",
        "                if output < predVal:\n",
        "                    pred = i\n",
        "                    predVal = output\n",
        "            #print(label)\n",
        "            label = label.to(device)\n",
        "            if pred == label:\n",
        "                correct += 1\n",
        "                #acc_category[category.numpy()[0]] += 1\n",
        "            count += 1\n",
        "            if count % 20 == 0:\n",
        "                print(\"Current Count is: {}\".format(count))\n",
        "                print('Accuracy on n way: {}'.format(correct/count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukYc6MZgBy3c"
      },
      "source": [
        "testSize = 200\n",
        "test_set = NWayOneShotEvalSet(testSize,transformations)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, num_workers = 2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0PlRLBvB7xK"
      },
      "source": [
        "import torch.optim as optim\n",
        "load_model = Net().to(device)\n",
        "load_optimizer = optim.SGD(load_model.parameters(), lr=0.0005)\n",
        "\n",
        "\n",
        "#num_epochs = 10\n",
        "#eval_every = 1000\n",
        "#total_step = len(train_loader)*num_epochs\n",
        "best_val_loss = load_checkpoint(load_model, load_optimizer)\n",
        "\n",
        "print(best_val_loss)\n",
        "eval(load_model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}